home.dir=${home.dir}

user.to.proxy=${user.to.proxy}
hadoop.job.ugi=${hadoop.job.ugi}

notify.emails=${notify.emails}
mail.sender=azkaban-noreply@linkedin.com

hadoop.home=/export/apps/hadoop/latest
pig.home=/export/apps/pig/littlepiggy-0.10-li0

default.udf.import.list=org.apache.pig.builtin.:oink.:com.linkedin.pig.:com.linkedin.pig.date.:org.apache.pig.piggybank.:org.apache.pig.piggybank.evaluation.:org.apache.pig.piggybank.evaluation.math.:org.apache.pig.piggybank.evaluation.datetime.:org.apache.pig.piggybank.evaluation.datetime.truncate.:org.apache.pig.piggybank.evaluation.datetime.diff.:org.apache.pig.piggybank.evaluation.datetime.convert.:org.apache.pig.piggybank.evaluation.stats.:org.apache.pig.piggybank.evaluation.string.:org.apache.pig.piggybank.evaluation.util.:org.apache.pig.piggybank.evaluation.decode.:org.apache.pig.piggybank.storage.:org.apache.pig.piggybank.storage.avro.:org.apache.pig.piggybank.storage.apachelog.:org.apache.pig.builtin.:datafu.linkanalysis.:datafu.pig.bags.:datafu.pig.bags.sets:datafu.pig.date.:datafu.pig.geo.:datafu.pig.hash.:datafu.pig.linkanalysis.:datafu.pig.numbers.:datafu.pig.sessions.:datafu.pig.stats.:datafu.pig.urls.:datafu.pig.util.

# add additional udf import list
udf.import.list=${default.udf.import.list}:com.linkedin.metronome.demo.pig.udf.

default.pig.additional.jars=${pig.home}/lib/*

# add additional pig jars
pig.additional.jars=./lib/JYMBII-0.0.1-SNAPSHOT.jar:${default.pig.additional.jars}

jvm.args=-Djava.io.tmpdir=/grid/a/mapred/tmp -Ddfs.umaskmode=002 -Dpig.additional.jars=${pig.additional.jars} -Dmapred.job.queue.name=${queue.name}

classpath=./*:./lib/*:${hadoop.home}/*:${hadoop.home}/lib/*:${pig.home}/lib/*
hdfs.default.classpath.dir=${remote.library.path}

hadoop-conf.mapred.output.compress=true
hadoop-conf.avro.mapred.deflate.level=6

hadoop-conf.mapred.job.queue.name=${queue.name}

queue.name=marathon
force.output.overwrite=true
