dependencies=train-0,train-1,train-2,train-3,train-4,train-5,train-6,train-7,train-8,train-9,train-10,train-11,train-12,train-13,train-14,train-15,train-16,train-17,train-18,train-19,train-20,train-21,train-22,train-23,train-24,train-25,train-26,train-27,train-28,train-29

#Job properties
type=pig

hadoop.job.ugi=${hadoop.job.ugi.cmd}
udf.import.list=oink.:com.linkedin.pig.:com.linkedin.pig.date.:org.apache.pig.piggybank.:org.apache.pig.piggybank.evaluation.:org.apache.pig.piggybank.evaluation.string.:org.apache.pig.piggybank.evaluation.math.:com.linkedin.liar.pigUDFs.
#pig.additional.jars=${project.artifactId}-${project.version}.jar

pig.script=Data-Collect.pig
# Arguments for optimizing Pig on Hadoop
#jvm.args=-Dmapred.job.queue.name=marathon -Djava.io.tmpdir=/grid/a/mapred/tmp -Ddfs.umaskmode=002 -Dpig.additional.jars=${pig.home}/lib/*:./${project.artifactId}-${project.version}.jar
#jvm.args=-Djava.io.tmpdir=/grid/a/mapred/tmp -Ddfs.umaskmode=002 -Dpig.additional.jars=${pig.home}/lib/*:./${project.artifactId}-${project.version}.jar

# Azkaban parameters
azkaban.should.proxy=${azkaban.should.proxy}
user.to.proxy=${user.to.proxy}
hdfs.default.classpath.dir=${remote.library.path}
#classpath=${local.classpath}


param.OUTPUT=${dcrr.base}
